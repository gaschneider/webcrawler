# Crawler for URLs in websites

## How to use

Clone the project and run through your CLI using as parameters

1. URL to crawl links
2. Max concurrency process (optional, default 2)
3. Max pages to crawl, make the crawl stop after the amount is reach(optional, default 10)